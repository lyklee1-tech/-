"""
ğŸ”¥ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ê¸°
7ì‹œê°„ ì´ë‚´ ê¸‰ìƒìŠ¹ ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì£¼ì œ ì¶”ì²œ
"""
import os
import sys
import json
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from loguru import logger
import re


class TrendAnalyzer:
    """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        logger.info("íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_google_trends(self, limit: int = 10) -> List[Dict]:
        """
        Google ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ìˆ˜ì§‘
        """
        try:
            logger.info("ğŸ” Google Trends ìˆ˜ì§‘ ì¤‘...")
            
            # Google Trends RSS API ì‚¬ìš©
            url = "https://trends.google.com/trends/trendingsearches/daily/rss?geo=KR"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # XML íŒŒì‹± (ê°„ë‹¨í•œ ì •ê·œì‹ ì‚¬ìš©)
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            trends = []
            for item in root.findall('.//item')[:limit]:
                title_elem = item.find('title')
                traffic_elem = item.find('.//ht:approx_traffic', {'ht': 'https://trends.google.com/trends/trendingsearches/daily'})
                
                if title_elem is not None:
                    title = title_elem.text
                    traffic = traffic_elem.text if traffic_elem is not None else 'N/A'
                    
                    trends.append({
                        'keyword': title,
                        'source': 'Google Trends',
                        'traffic': traffic,
                        'score': self._calculate_score(traffic, 'google'),
                        'timestamp': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… Google Trends: {len(trends)}ê°œ ìˆ˜ì§‘")
            return trends
            
        except Exception as e:
            logger.error(f"âŒ Google Trends ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def get_youtube_trends(self, limit: int = 10) -> List[Dict]:
        """
        YouTube ì¸ê¸° ê¸‰ìƒìŠ¹ ë™ì˜ìƒ ì£¼ì œ ìˆ˜ì§‘
        """
        try:
            logger.info("ğŸ¥ YouTube Trends ìˆ˜ì§‘ ì¤‘...")
            
            # YouTube Data API í•„ìš” (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            api_key = os.getenv('YOUTUBE_API_KEY')
            
            if not api_key:
                logger.warning("YouTube API Key ì—†ìŒ - ê¸°ë³¸ íŠ¸ë Œë“œ ì‚¬ìš©")
                return self._get_youtube_fallback()
            
            url = "https://www.googleapis.com/youtube/v3/videos"
            params = {
                'part': 'snippet,statistics',
                'chart': 'mostPopular',
                'regionCode': 'KR',
                'videoCategoryId': '25',  # ë‰´ìŠ¤ & ì •ì¹˜
                'maxResults': limit,
                'key': api_key
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            trends = []
            for item in data.get('items', []):
                snippet = item['snippet']
                stats = item['statistics']
                
                # ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                if self._is_economic_topic(snippet['title']):
                    trends.append({
                        'keyword': snippet['title'],
                        'source': 'YouTube',
                        'views': int(stats.get('viewCount', 0)),
                        'score': self._calculate_score(stats.get('viewCount', 0), 'youtube'),
                        'timestamp': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… YouTube: {len(trends)}ê°œ ìˆ˜ì§‘")
            return trends
            
        except Exception as e:
            logger.error(f"âŒ YouTube Trends ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._get_youtube_fallback()
    
    def get_naver_trends(self, limit: int = 10) -> List[Dict]:
        """
        ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ìˆ˜ì§‘
        """
        try:
            logger.info("ğŸ” ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì¤‘...")
            
            # ë„¤ì´ë²„ DataLab API (ë˜ëŠ” í¬ë¡¤ë§)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ êµ¬í˜„
            
            url = "https://datalab.naver.com/keyword/realtimeList.naver"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            
            # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            trends = []
            
            # ë„¤ì´ë²„ APIê°€ ì œí•œì ì´ë¯€ë¡œ fallback ë°ì´í„° ì‚¬ìš©
            return self._get_naver_fallback()
            
        except Exception as e:
            logger.error(f"âŒ ë„¤ì´ë²„ Trends ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._get_naver_fallback()
    
    def get_economic_keywords(self) -> List[str]:
        """ê²½ì œ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ"""
        return [
            'ì£¼ì‹', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤',
            'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ì•”í˜¸í™”í', 'ê°€ìƒí™”í',
            'í™˜ìœ¨', 'ë‹¬ëŸ¬', 'ì›í™”', 'ì—”í™”',
            'ê¸ˆë¦¬', 'í•œêµ­ì€í–‰', 'ê¸°ì¤€ê¸ˆë¦¬', 'ë¯¸êµ­ ê¸ˆë¦¬',
            'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì§‘ê°’',
            'ê²½ì œ', 'ì¦ì‹œ', 'ì¦ê¶Œ', 'íˆ¬ì',
            'ë°˜ë„ì²´', 'AI', 'ì¸ê³µì§€ëŠ¥',
            'í…ŒìŠ¬ë¼', 'ì• í”Œ', 'NVIDIA', 'ì—”ë¹„ë””ì•„',
            'S&P500', 'ë‚˜ìŠ¤ë‹¥', 'ë‹¤ìš°ì¡´ìŠ¤',
            'ìœ ê°€', 'ì›ìœ ', 'ê¸ˆê°’',
            'ì‹¤ì—…ë¥ ', 'GDP', 'ë¬¼ê°€',
            'ì¸í”Œë ˆì´ì…˜', 'ë””í”Œë ˆì´ì…˜'
        ]
    
    def search_economic_news(self, keywords: List[str], hours: int = 7) -> List[Dict]:
        """
        íŠ¹ì • í‚¤ì›Œë“œë¡œ ìµœê·¼ Nì‹œê°„ ì´ë‚´ ê²½ì œ ë‰´ìŠ¤ ê²€ìƒ‰
        """
        try:
            logger.info(f"ğŸ“° ê²½ì œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘... (ìµœê·¼ {hours}ì‹œê°„)")
            
            news_items = []
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API
            naver_client_id = os.getenv('NAVER_CLIENT_ID')
            naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
            
            if not naver_client_id or not naver_client_secret:
                logger.warning("ë„¤ì´ë²„ API í‚¤ ì—†ìŒ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                return self._get_news_fallback()
            
            for keyword in keywords[:5]:  # ìƒìœ„ 5ê°œë§Œ
                url = "https://openapi.naver.com/v1/search/news.json"
                params = {
                    'query': keyword + ' ê²½ì œ',
                    'display': 10,
                    'sort': 'date'
                }
                headers = {
                    'X-Naver-Client-Id': naver_client_id,
                    'X-Naver-Client-Secret': naver_client_secret
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    for item in data.get('items', []):
                        news_items.append({
                            'keyword': keyword,
                            'title': self._clean_html(item['title']),
                            'description': self._clean_html(item['description']),
                            'link': item['link'],
                            'source': 'Naver News',
                            'pubDate': item['pubDate'],
                            'score': 50  # ê¸°ë³¸ ì ìˆ˜
                        })
            
            logger.info(f"âœ… ë‰´ìŠ¤: {len(news_items)}ê°œ ìˆ˜ì§‘")
            return news_items
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._get_news_fallback()
    
    def analyze_all_trends(self, hours: int = 7) -> Dict:
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ íŠ¸ë Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„
        """
        current_time = datetime.now()
        logger.info("=" * 80)
        logger.info("ğŸ”¥ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        logger.info(f"ğŸ“… í˜„ì¬ ì‹œê°: {current_time.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}")
        logger.info(f"â° ë¶„ì„ ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„")
        logger.info(f"ğŸ• ê¸°ì¤€ ì‹œê°„: {(current_time - timedelta(hours=hours)).strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')} ~ í˜„ì¬")
        logger.info("=" * 80)
        
        all_trends = []
        
        # 1. Google Trends
        google_trends = self.get_google_trends()
        all_trends.extend(google_trends)
        
        # 2. YouTube Trends
        youtube_trends = self.get_youtube_trends()
        all_trends.extend(youtube_trends)
        
        # 3. ë„¤ì´ë²„ Trends
        naver_trends = self.get_naver_trends()
        all_trends.extend(naver_trends)
        
        # 4. ê²½ì œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
        economic_keywords = self.get_economic_keywords()
        news_items = self.search_economic_news(economic_keywords, hours)
        
        # 5. ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        all_trends.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        # 6. ì¤‘ë³µ ì œê±° ë° ê²½ì œ ê´€ë ¨ í•„í„°ë§
        unique_trends = self._deduplicate_and_filter(all_trends)
        
        # 7. ìƒìœ„ ì¶”ì²œ ì£¼ì œ ì„ ì •
        top_recommendations = unique_trends[:10]
        
        result = {
            'timestamp': datetime.now().isoformat(),
            'analysis_period_hours': hours,
            'total_trends_found': len(all_trends),
            'filtered_trends': len(unique_trends),
            'recommendations': top_recommendations,
            'news_items': news_items[:20],
            'sources': {
                'google': len(google_trends),
                'youtube': len(youtube_trends),
                'naver': len(naver_trends),
                'news': len(news_items)
            }
        }
        
        logger.info("=" * 80)
        logger.info(f"âœ… íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(all_trends)}ê°œ íŠ¸ë Œë“œ ìˆ˜ì§‘")
        logger.info(f"ğŸ¯ ì¶”ì²œ ì£¼ì œ: {len(top_recommendations)}ê°œ")
        logger.info("=" * 80)
        
        # ì¶”ì²œ ì£¼ì œ ì¶œë ¥
        logger.info("\nğŸ”¥ TOP 10 ì¶”ì²œ ì£¼ì œ:")
        for i, trend in enumerate(top_recommendations, 1):
            logger.info(f"{i}. {trend.get('keyword')} (ì ìˆ˜: {trend.get('score', 0)}) - {trend.get('source')}")
        
        return result
    
    def get_top_topic(self, hours: int = 7) -> Optional[str]:
        """
        ê°€ì¥ í•«í•œ ì£¼ì œ 1ê°œ ë°˜í™˜ (ìë™ ì„ íƒìš©)
        """
        result = self.analyze_all_trends(hours)
        
        if result['recommendations']:
            top_topic = result['recommendations'][0]['keyword']
            logger.info(f"ğŸ¯ ìë™ ì„ íƒëœ ì£¼ì œ: {top_topic}")
            return top_topic
        
        return None
    
    # ===== Helper ë©”ì„œë“œ =====
    
    def _calculate_score(self, value, source_type: str) -> int:
        """ì ìˆ˜ ê³„ì‚°"""
        try:
            if source_type == 'google':
                # íŠ¸ë˜í”½ ë¬¸ìì—´ íŒŒì‹± (ì˜ˆ: "100K+", "1M+")
                if isinstance(value, str):
                    value = value.replace('+', '').replace(',', '')
                    if 'K' in value:
                        return int(float(value.replace('K', '')) * 1000 / 100)
                    elif 'M' in value:
                        return int(float(value.replace('M', '')) * 1000000 / 100)
                return 50
            
            elif source_type == 'youtube':
                # ì¡°íšŒìˆ˜ ê¸°ë°˜
                views = int(value) if value else 0
                return min(100, views // 10000)  # 1ë§Œ ì¡°íšŒìˆ˜ë‹¹ 1ì 
            
            else:
                return 50
                
        except:
            return 50
    
    def _is_economic_topic(self, text: str) -> bool:
        """ê²½ì œ ê´€ë ¨ ì£¼ì œì¸ì§€ íŒë‹¨"""
        economic_keywords = self.get_economic_keywords()
        text_lower = text.lower()
        
        for keyword in economic_keywords:
            if keyword.lower() in text_lower:
                return True
        
        return False
    
    def _deduplicate_and_filter(self, trends: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±° ë° ê²½ì œ ê´€ë ¨ í•„í„°ë§"""
        seen = set()
        filtered = []
        
        for trend in trends:
            keyword = trend.get('keyword', '')
            
            # ì¤‘ë³µ ì²´í¬
            if keyword in seen:
                continue
            
            # ê²½ì œ ê´€ë ¨ ì£¼ì œë§Œ í•„í„°ë§
            if self._is_economic_topic(keyword):
                seen.add(keyword)
                filtered.append(trend)
        
        return filtered
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        import re
        return re.sub(r'<[^>]+>', '', text)
    
    # ===== Fallback ë°ì´í„° =====
    
    def _get_youtube_fallback(self) -> List[Dict]:
        """YouTube API ì—†ì„ ë•Œ ê¸°ë³¸ ë°ì´í„°"""
        return [
            {'keyword': 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'source': 'YouTube', 'views': 500000, 'score': 80},
            {'keyword': 'ì‚¼ì„±ì „ì ì‹¤ì ', 'source': 'YouTube', 'views': 300000, 'score': 70},
            {'keyword': 'ë¯¸êµ­ ê¸ˆë¦¬ ì¸ìƒ', 'source': 'YouTube', 'views': 250000, 'score': 65},
        ]
    
    def _get_naver_fallback(self) -> List[Dict]:
        """ë„¤ì´ë²„ API ì—†ì„ ë•Œ ê¸°ë³¸ ë°ì´í„°"""
        return [
            {'keyword': 'ì½”ìŠ¤í”¼ ìƒìŠ¹', 'source': 'Naver', 'traffic': '50K+', 'score': 75},
            {'keyword': 'ë‹¬ëŸ¬ í™˜ìœ¨', 'source': 'Naver', 'traffic': '30K+', 'score': 60},
        ]
    
    def _get_news_fallback(self) -> List[Dict]:
        """ë‰´ìŠ¤ API ì—†ì„ ë•Œ ê¸°ë³¸ ë°ì´í„°"""
        return [
            {
                'keyword': 'ì£¼ì‹',
                'title': 'ì½”ìŠ¤í”¼, ì™¸êµ­ì¸ ë§¤ìˆ˜ì„¸ì— ìƒìŠ¹ì„¸',
                'description': 'ì½”ìŠ¤í”¼ê°€ ì™¸êµ­ì¸ íˆ¬ììë“¤ì˜ ë§¤ìˆ˜ì„¸ì— í˜ì…ì–´ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.',
                'source': 'Sample News',
                'score': 70
            }
        ]


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    analyzer = TrendAnalyzer()
    
    # ì „ì²´ ë¶„ì„
    result = analyzer.analyze_all_trends(hours=7)
    
    # JSONìœ¼ë¡œ ì €ì¥
    output_path = 'data/trends/latest_trends.json'
    os.makedirs('data/trends', exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìë™ ì¶”ì²œ ì£¼ì œ
    top_topic = analyzer.get_top_topic()
    logger.info(f"\nğŸ¯ ì¶”ì²œ ì£¼ì œ: {top_topic}")


if __name__ == '__main__':
    main()

"""
Banana ëª¨ë“œ ë¹„ë””ì˜¤ ìƒì„±ê¸° - AI ì´ë¯¸ì§€ ìë™ ìƒì„± + ì¥ë©´ ë¶„í•  + ë£¨í”„ ì§€ì›
ì˜ìƒ ê¸¸ì´: 20ì´ˆ ~ 30ë¶„
"""
import os
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import numpy as np
from moviepy.editor import (
    VideoFileClip, ImageClip, AudioFileClip, TextClip,
    CompositeVideoClip, concatenate_videoclips, ColorClip
)
from moviepy.video.fx.all import resize, fadein, fadeout, crop
import yaml
from loguru import logger
from openai import OpenAI
from PIL import Image
import requests
from io import BytesIO


class BananaVideoCreator:
    """Banana ìŠ¤íƒ€ì¼ ë¹„ë””ì˜¤ ìƒì„±ê¸° - ì™„ì „ ìë™í™”"""
    
    def __init__(self, config_path='config/config.yaml'):
        # ì„¤ì • ë¡œë“œ
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            self.video_config = config['video']
            self.subtitle_config = config['subtitles']
            self.background_config = config['background']
            self.banana_config = config.get('banana_mode', {})
        
        # OpenAI API ì´ˆê¸°í™”
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=self.openai_api_key)
        else:
            self.openai_client = None
        
        # í•´ìƒë„ ì„¤ì •
        width, height = map(int, self.video_config['resolution'].split('x'))
        self.width = width
        self.height = height
        self.fps = self.video_config['fps']
        
        logger.info("ğŸŒ Banana ëª¨ë“œ ë¹„ë””ì˜¤ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_banana_video(
        self,
        topic: str,
        script_text: str,
        audio_path: str,
        output_path: str,
        target_duration: Optional[int] = None,
        style: str = 'professional'
    ) -> bool:
        """
        Banana ìŠ¤íƒ€ì¼ ë¹„ë””ì˜¤ ìƒì„± (AutoPilot ëª¨ë“œ)
        
        Args:
            topic: ë¹„ë””ì˜¤ í† í”½/ì£¼ì œ
            script_text: ì „ì²´ ìŠ¤í¬ë¦½íŠ¸
            audio_path: TTS ì˜¤ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            target_duration: ëª©í‘œ ê¸¸ì´ (ì´ˆ, Noneì´ë©´ ì˜¤ë””ì˜¤ ê¸¸ì´ ì‚¬ìš©)
            style: ìŠ¤íƒ€ì¼ í…œí”Œë¦¿ (professional, cinematic, anime, 3d)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸŒ Banana ëª¨ë“œ ì‹œì‘ - í† í”½: {topic}")
            
            # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
            narration = AudioFileClip(audio_path)
            audio_duration = narration.duration
            
            if target_duration is None:
                target_duration = int(audio_duration)
            
            logger.info(f"ğŸ“ ëª©í‘œ ì˜ìƒ ê¸¸ì´: {target_duration}ì´ˆ ({target_duration // 60}ë¶„ {target_duration % 60}ì´ˆ)")
            
            # 2. ìŠ¤í¬ë¦½íŠ¸ ì¥ë©´ ë¶„í• 
            scenes = self._segment_script(script_text, target_duration)
            logger.info(f"ğŸ¬ ì´ {len(scenes)}ê°œ ì¥ë©´ ìƒì„±")
            
            # 3. ê° ì¥ë©´ë³„ AI ì´ë¯¸ì§€ ìƒì„±
            scene_clips = []
            for i, scene in enumerate(scenes):
                logger.info(f"ğŸ¨ ì¥ë©´ {i+1}/{len(scenes)} ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
                scene_clip = self._create_scene_with_ai_image(
                    scene, topic, style
                )
                scene_clips.append(scene_clip)
            
            # 4. ì¥ë©´ ì—°ê²°
            logger.info("ğŸ”— ì¥ë©´ ì—°ê²° ì¤‘...")
            video = concatenate_videoclips(scene_clips, method="compose")
            
            # 5. ìë§‰ ì¶”ê°€
            logger.info("ğŸ“ ìë§‰ ìƒì„± ì¤‘...")
            subtitles = self._create_subtitles(script_text, video.duration)
            
            # 6. í•©ì„±
            final_clips = [video] + subtitles
            
            # 7. íˆ¬ì ì±…ì„ ë¬¸êµ¬ ì¶”ê°€
            if self.video_config.get('disclaimer', {}).get('enabled', True):
                disclaimer = self._create_disclaimer(video.duration)
                final_clips.append(disclaimer)
            
            final_video = CompositeVideoClip(final_clips, size=(self.width, self.height))
            
            # 8. ì˜¤ë””ì˜¤ ë¯¹ì‹±
            final_audio = self._mix_audio(narration, video.duration)
            final_video = final_video.set_audio(final_audio)
            
            # 9. ë¹„ë””ì˜¤ ì¶œë ¥
            logger.info(f"ğŸ’¾ ë¹„ë””ì˜¤ ë Œë”ë§ ì¤‘... ({output_path})")
            final_video.write_videofile(
                output_path,
                fps=self.fps,
                codec=self.video_config['codec'],
                audio_codec=self.video_config['audio_codec'],
                preset='medium',
                threads=4,
                logger=None
            )
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            final_video.close()
            for clip in scene_clips:
                clip.close()
            narration.close()
            
            logger.info(f"âœ… Banana ëª¨ë“œ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Banana ëª¨ë“œ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def _segment_script(self, script: str, target_duration: int) -> List[Dict]:
        """
        ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¥ë©´ë³„ë¡œ ìë™ ë¶„í• 
        
        Args:
            script: ì „ì²´ ìŠ¤í¬ë¦½íŠ¸
            target_duration: ëª©í‘œ ì˜ìƒ ê¸¸ì´ (ì´ˆ)
        
        Returns:
            ì¥ë©´ ë¦¬ìŠ¤íŠ¸ [{'text': str, 'duration': float, 'description': str}, ...]
        """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = [s.strip() for s in script.split('.') if s.strip()]
        
        # ì¥ë©´ ì„¤ì •
        scene_config = self.banana_config.get('scene_segmentation', {})
        min_duration = scene_config.get('min_scene_duration', 3)
        max_duration = scene_config.get('max_scene_duration', 10)
        
        # í‰ê·  ì¥ë©´ ê¸¸ì´ ê³„ì‚°
        num_scenes = max(1, len(sentences))
        avg_duration = target_duration / num_scenes
        
        # ì¥ë©´ ìƒì„±
        scenes = []
        for i, sentence in enumerate(sentences):
            # ê° ì¥ë©´ì˜ ê¸¸ì´ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°
            duration = min(max_duration, max(min_duration, avg_duration))
            
            scenes.append({
                'text': sentence,
                'duration': duration,
                'description': self._generate_scene_description(sentence),
                'index': i
            })
        
        return scenes
    
    def _generate_scene_description(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ AI ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
        
        Args:
            text: ì¥ë©´ í…ìŠ¤íŠ¸
        
        Returns:
            ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
        """
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        keywords = {
            'ë¹„íŠ¸ì½”ì¸': 'Bitcoin cryptocurrency chart',
            'ì£¼ì‹': 'stock market trading floor',
            'ê²½ì œ': 'modern financial district',
            'íˆ¬ì': 'investment portfolio dashboard',
            'ê¸‰ë“±': 'rising green chart arrow',
            'ê¸‰ë½': 'falling red chart',
            'í™˜ìœ¨': 'currency exchange rates',
            'ê¸ˆë¦¬': 'interest rate graph',
            'ì‹œì¥': 'bustling stock exchange',
            'ê¸°ì—…': 'modern office building',
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword, description in keywords.items():
            if keyword in text:
                return description
        
        # ê¸°ë³¸ ì„¤ëª…
        return 'professional business background with financial elements'
    
    def _create_scene_with_ai_image(
        self,
        scene: Dict,
        topic: str,
        style: str
    ) -> ImageClip:
        """
        AI ì´ë¯¸ì§€ë¡œ ì¥ë©´ ìƒì„±
        
        Args:
            scene: ì¥ë©´ ì •ë³´
            topic: ë¹„ë””ì˜¤ í† í”½
            style: ìŠ¤íƒ€ì¼ í…œí”Œë¦¿
        
        Returns:
            ImageClip
        """
        try:
            # AI ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
            ai_config = self.background_config.get('ai_image', {})
            
            if not ai_config.get('enabled', False) or not self.openai_client:
                # AI ìƒì„± ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ë°°ê²½ ì‚¬ìš©
                logger.warning("âš ï¸ AI ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™” - ê¸°ë³¸ ë°°ê²½ ì‚¬ìš©")
                return self._create_default_background(scene['duration'])
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_template = ai_config.get('prompt_template', '')
            prompt = prompt_template.format(
                style=style,
                description=scene['description']
            )
            
            logger.info(f"ğŸ¨ AI ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt[:100]}...")
            
            # DALL-E 3ë¡œ ì´ë¯¸ì§€ ìƒì„± (OpenAI v1.0+ API)
            response = self.openai_client.images.generate(
                model=ai_config.get('model', 'dall-e-3'),
                prompt=prompt,
                size=ai_config.get('size', '1024x1792'),
                quality=ai_config.get('quality', 'hd'),
                style=ai_config.get('style', 'vivid'),
                n=1
            )
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            image_url = response.data[0].url
            img_response = requests.get(image_url)
            img = Image.open(BytesIO(img_response.content))
            
            # ì„ì‹œ ì €ì¥
            temp_dir = Path('data/temp/ai_images')
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = int(time.time() * 1000)
            img_path = temp_dir / f"scene_{scene['index']}_{timestamp}.png"
            img.save(img_path)
            
            logger.info(f"âœ… AI ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {img_path}")
            
            # ImageClip ìƒì„±
            clip = ImageClip(str(img_path)).set_duration(scene['duration'])
            
            # Ken Burns íš¨ê³¼ (ì¤Œ ì• ë‹ˆë©”ì´ì…˜)
            anim_config = ai_config.get('animation', {})
            if anim_config.get('enabled', True):
                clip = self._apply_ken_burns_effect(
                    clip,
                    zoom_factor=anim_config.get('zoom_factor', 1.2)
                )
            
            # í™”ë©´ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
            clip = clip.resize((self.width, self.height))
            
            return clip
            
        except Exception as e:
            logger.error(f"âŒ AI ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ë°°ê²½ ì‚¬ìš©
            return self._create_default_background(scene['duration'])
    
    def _create_default_background(self, duration: float) -> ColorClip:
        """ê¸°ë³¸ ê·¸ë¼ë°ì´ì…˜ ë°°ê²½ ìƒì„±"""
        gradient_colors = self.background_config.get('gradient', {}).get('colors', ['#0f2027', '#203a43', '#2c5364'])
        color = gradient_colors[0]
        return ColorClip(
            size=(self.width, self.height),
            color=self._hex_to_rgb(color)
        ).set_duration(duration)
    
    def _apply_ken_burns_effect(self, clip: ImageClip, zoom_factor: float = 1.2) -> ImageClip:
        """
        Ken Burns íš¨ê³¼ (ì¤Œ + íŒ¬ ì• ë‹ˆë©”ì´ì…˜)
        
        Args:
            clip: ì›ë³¸ í´ë¦½
            zoom_factor: ì¤Œ ë°°ìœ¨
        
        Returns:
            ì• ë‹ˆë©”ì´ì…˜ ì ìš©ëœ í´ë¦½
        """
        def zoom(t):
            # ì‹œê°„ì— ë”°ë¼ ì¤Œ ë¹„ìœ¨ ì¦ê°€
            progress = t / clip.duration
            current_zoom = 1 + (zoom_factor - 1) * progress
            return current_zoom
        
        return clip.resize(lambda t: zoom(t))
    
    def _create_subtitles(self, text: str, duration: float) -> List[TextClip]:
        """ìë§‰ ìƒì„± (ë‹¨ì–´ ë‹¨ìœ„ ì• ë‹ˆë©”ì´ì…˜)"""
        subtitles = []
        words = text.split()
        
        if not words:
            return subtitles
        
        # íƒ€ì´ë° ê³„ì‚°
        words_per_second = self.subtitle_config['timing'].get('words_per_second', 3)
        word_duration = 1.0 / words_per_second
        
        # ìë§‰ ìŠ¤íƒ€ì¼
        font = self.subtitle_config.get('font', 'Arial-Bold')
        fontsize = self.subtitle_config.get('font_size', 60)
        color = self.subtitle_config.get('font_color', 'white')
        
        current_time = 0
        for i, word in enumerate(words):
            if current_time >= duration:
                break
            
            # í…ìŠ¤íŠ¸ í´ë¦½ ìƒì„±
            txt_clip = TextClip(
                word,
                fontsize=fontsize,
                font=font,
                color=color,
                stroke_color=self.subtitle_config.get('outline_color', 'black'),
                stroke_width=self.subtitle_config.get('outline_width', 3)
            ).set_position('center').set_start(current_time).set_duration(word_duration * 1.5)
            
            subtitles.append(txt_clip)
            current_time += word_duration
        
        return subtitles
    
    def _create_disclaimer(self, duration: float) -> TextClip:
        """íˆ¬ì ì±…ì„ ë¬¸êµ¬ ìƒì„±"""
        disclaimer_config = self.video_config.get('disclaimer', {})
        
        text = disclaimer_config.get('text', 'ë³¸ ì˜ìƒì€ íˆ¬ì ì°¸ê³ ìš©ì´ë©°\níˆ¬ì ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤')
        fontsize = disclaimer_config.get('fontsize', 24)
        color = disclaimer_config.get('color', '#cccccc')
        
        txt_clip = TextClip(
            text,
            fontsize=fontsize,
            font=self.subtitle_config.get('font', 'Arial-Bold'),
            color=color,
            method='caption',
            size=(self.width - 100, None),
            align='center'
        ).set_position(('center', self.height - 150)).set_duration(duration)
        
        return txt_clip
    
    def _mix_audio(self, narration: AudioFileClip, video_duration: float) -> AudioFileClip:
        """ì˜¤ë””ì˜¤ ë¯¹ì‹± (ë‚˜ë ˆì´ì…˜ + BGM + SFX)"""
        # ê°„ë‹¨ ë²„ì „: ë‚˜ë ˆì´ì…˜ë§Œ ë°˜í™˜
        # TODO: BGM ë° SFX ì¶”ê°€
        return narration
    
    def _hex_to_rgb(self, hex_color: str) -> tuple:
        """HEX ìƒ‰ìƒì„ RGB íŠœí”Œë¡œ ë³€í™˜"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    
    def create_thumbnail(self, topic: str, output_path: str) -> bool:
        """
        AI ì¸ë„¤ì¼ ìë™ ìƒì„±
        
        Args:
            topic: ë¹„ë””ì˜¤ í† í”½
            output_path: ì¸ë„¤ì¼ ì €ì¥ ê²½ë¡œ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            thumbnail_config = self.banana_config.get('thumbnail_generation', {})
            
            if not thumbnail_config.get('enabled', False) or not self.openai_client:
                logger.warning("âš ï¸ AI ì¸ë„¤ì¼ ìƒì„± ë¹„í™œì„±í™”")
                return False
            
            # ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"YouTube thumbnail for video about {topic}. Eye-catching, professional, high contrast, bold text overlay. 16:9 aspect ratio."
            
            logger.info(f"ğŸ¨ AI ì¸ë„¤ì¼ ìƒì„± ì¤‘... (í† í”½: {topic})")
            
            # DALL-E 3ë¡œ ì¸ë„¤ì¼ ìƒì„± (OpenAI v1.0+ API)
            response = self.openai_client.images.generate(
                model='dall-e-3',
                prompt=prompt,
                size='1792x1024',  # 16:9 ë¹„ìœ¨
                quality='hd',
                n=1
            )
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
            image_url = response.data[0].url
            img_response = requests.get(image_url)
            img = Image.open(BytesIO(img_response.content))
            
            # ì¸ë„¤ì¼ í¬ê¸° ì¡°ì • (YouTube ê¶Œì¥ í¬ê¸°)
            thumb_width = self.video_config['thumbnail']['width']
            thumb_height = self.video_config['thumbnail']['height']
            img = img.resize((thumb_width, thumb_height), Image.LANCZOS)
            
            img.save(output_path)
            
            logger.info(f"âœ… AI ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ AI ì¸ë„¤ì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
